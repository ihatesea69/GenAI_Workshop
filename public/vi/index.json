[
{
	"uri": "//localhost:1313/vi/1-token-limitations-in-llms/",
	"title": "Gi·ªõi h·∫°n Token trong Large Language Models",
	"tags": [],
	"description": "",
	"content": "V·∫•n ƒë·ªÅ Large Language Models (LLMs) g·∫∑p h·∫°n ch·∫ø ƒë√°ng k·ªÉ v·ªÅ gi·ªõi h·∫°n token, g√¢y kh√≥ khƒÉn trong vi·ªác x·ª≠ l√Ω c√°c t√†i li·ªáu l·ªõn (v√≠ d·ª•: b√°o c√°o 20 trang) m·ªôt c√°ch hi·ªáu qu·∫£. H·∫°n ch·∫ø n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn kh·∫£ nƒÉng hi·ªÉu v√† t·∫°o ph·∫£n h·ªìi d·ª±a tr√™n ng·ªØ c·∫£nh l·ªõn c·ªßa m√¥ h√¨nh.\nGi·∫£i ph√°p 1. Chi·∫øn l∆∞·ª£c \u0026ldquo;Chia ƒê·ªÉ Tr·ªã\u0026rdquo; text_splitter = CharacterTextSplitter( separator=\u0026#34;\\n\u0026#34;, chunk_size=512, chunk_overlap=50, length_function=len ) K√≠ch th∆∞·ªõc chunk: 512 k√Ω t·ª± m·ªói chunk ƒê·ªô ch·ªìng l·∫•p: 50 k√Ω t·ª± ƒë·ªÉ duy tr√¨ t√≠nh li√™n t·ª•c ng·ªØ c·∫£nh D·∫•u ph√¢n t√°ch: K√Ω t·ª± xu·ªëng d√≤ng cho c√°c ƒëi·ªÉm ng·∫Øt t·ª± nhi√™n 2. Tri·ªÉn khai Vector Storage embeddings = OpenAIEmbeddings() vectorstore = FAISS.from_texts(chunks, embeddings) S·ª≠ d·ª•ng m√¥ h√¨nh embedding c·ªßa OpenAI FAISS cho t√¨m ki·∫øm t∆∞∆°ng ƒë·ªìng hi·ªáu qu·∫£ L∆∞u tr·ªØ vector trong b·ªô nh·ªõ ƒë·ªÉ truy xu·∫•t nhanh 3. RAG (Retrieval Augmented Generation) qa_chain = RetrievalQA.from_chain_type( llm=OpenAI(), chain_type=\u0026#34;stuff\u0026#34;, retriever=vectorstore.as_retriever(search_kwargs={\u0026#34;k\u0026#34;: 3}) ) Truy xu·∫•t 3 chunk li√™n quan nh·∫•t K·∫øt h·ª£p c√°c chunk ƒë·ªÉ t·∫°o ph·∫£n h·ªìi c√≥ ng·ªØ c·∫£nh S·ª≠ d·ª•ng LLM c·ªßa OpenAI ƒë·ªÉ t·∫°o ph·∫£n h·ªìi T√≠nh nƒÉng X·ª≠ l√Ω T√†i li·ªáu H·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng file (TXT, PDF, DOCX) Ch·ª©c nƒÉng xem tr∆∞·ªõc t√†i li·ªáu Theo d√µi ti·∫øn tr√¨nh x·ª≠ l√Ω X·ª≠ l√Ω Truy v·∫•n Nh·∫≠p c√¢u h·ªèi t∆∞∆°ng t√°c Hi·ªÉn th·ªã c√°c chunk t√†i li·ªáu li√™n quan Ph·∫£n h·ªìi AI d·ª±a tr√™n ng·ªØ c·∫£nh Giao di·ªán Ng∆∞·ªùi d√πng Giao di·ªán Streamlit tr·ª±c quan Ch·ªâ b√°o ti·∫øn tr√¨nh Ph·∫ßn k·∫øt qu·∫£ c√≥ th·ªÉ m·ªü r·ªông V√≠ d·ª• Tri·ªÉn khai # Upload v√† x·ª≠ l√Ω t√†i li·ªáu uploaded_file = st.file_uploader(\u0026#34;üìÑ T·∫£i l√™n t√†i li·ªáu\u0026#34;, type=[\u0026#34;txt\u0026#34;, \u0026#34;pdf\u0026#34;, \u0026#34;docx\u0026#34;]) if uploaded_file is not None: text = uploaded_file.read().decode(\u0026#34;utf-8\u0026#34;) # X·ª≠ l√Ω chunk chunks = text_splitter.split_text(text) # L∆∞u tr·ªØ vector vectorstore = FAISS.from_texts(chunks, embeddings) # X·ª≠ l√Ω truy v·∫•n query = st.text_input(\u0026#34;ƒê·∫∑t c√¢u h·ªèi c·ªßa b·∫°n:\u0026#34;) if query: docs = vectorstore.similarity_search(query, k=3) response = qa_chain.run(query) Best Practices Ti·ªÅn x·ª≠ l√Ω\nL√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n tr∆∞·ªõc khi chia nh·ªè Lo·∫°i b·ªè n·ªôi dung kh√¥ng li√™n quan ƒë·ªÉ t·ªëi ∆∞u h√≥a token Duy tr√¨ c·∫•u tr√∫c t√†i li·ªáu khi c√≥ th·ªÉ Chi·∫øn l∆∞·ª£c Chia nh·ªè\nƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc chunk d·ª±a tr√™n lo·∫°i n·ªôi dung S·ª≠ d·ª•ng ƒë·ªô ch·ªìng l·∫•p ph√π h·ª£p ƒë·ªÉ b·∫£o to√†n ng·ªØ c·∫£nh Xem x√©t ranh gi·ªõi ng·ªØ nghƒ©a khi c√≥ th·ªÉ T·ªëi ∆∞u h√≥a Truy v·∫•n\nTri·ªÉn khai cache cho c√°c truy v·∫•n th∆∞·ªùng xuy√™n T·ªëi ∆∞u s·ªë l∆∞·ª£ng chunk ƒë∆∞·ª£c truy xu·∫•t C√¢n b·∫±ng ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi v·ªõi t·ªëc ƒë·ªô x·ª≠ l√Ω C√¢n nh·∫Øc v·ªÅ Hi·ªáu nƒÉng S·ª≠ d·ª•ng B·ªô nh·ªõ\nGi√°m s√°t k√≠ch th∆∞·ªõc vector store Tri·ªÉn khai x·ª≠ l√Ω h√†ng lo·∫°t cho t√†i li·ªáu l·ªõn D·ªçn d·∫πp b·ªô nh·ªõ t·∫°m Th·ªùi gian Ph·∫£n h·ªìi\nCache c√°c truy v·∫•n ph·ªï bi·∫øn T·ªëi ∆∞u h√≥a truy xu·∫•t chunk C√¢n b·∫±ng k√≠ch th∆∞·ªõc chunk v·ªõi ƒë·ªô ch√≠nh x√°c ƒê·ªô ch√≠nh x√°c\nX√°c th·ª±c ph·∫£n h·ªìi v·ªõi t√†i li·ªáu ngu·ªìn ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc chunk d·ª±a tr√™n ƒë·ªô ph·ª©c t·∫°p n·ªôi dung Gi√°m s√°t v√† ghi log ch·∫•t l∆∞·ª£ng truy xu·∫•t "
},
{
	"uri": "//localhost:1313/vi/2-embedding-knowledge-systems/",
	"title": "H·ªá th·ªëng Embedding Knowledge",
	"tags": [],
	"description": "",
	"content": "V·∫•n ƒë·ªÅ C√°c h·ªá th·ªëng multi-agent th∆∞·ªùng g·∫∑p kh√≥ khƒÉn trong vi·ªác x·ª≠ l√Ω v√† hi·ªÉu d·ªØ li·ªáu c√≥ c·∫•u tr√∫c ngo√†i vƒÉn b·∫£n thu·∫ßn t√∫y. Th√°ch th·ª©c n√†y ƒë√≤i h·ªèi m·ªôt c√°ch ti·∫øp c·∫≠n th·ªëng nh·∫•t ƒë·ªÉ x·ª≠ l√Ω c√°c lo·∫°i d·ªØ li·ªáu kh√°c nhau trong khi v·∫´n duy tr√¨ s·ª± hi·ªÉu bi·∫øt ng·ªØ nghƒ©a gi·ªØa c√°c agent.\nKi·∫øn tr√∫c Gi·∫£i ph√°p 1. M√¥ h√¨nh Embedding Th·ªëng nh·∫•t @st.cache_resource def load_model(): tokenizer = AutoTokenizer.from_pretrained(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) model = AutoModel.from_pretrained(\u0026#34;sentence-transformers/all-MiniLM-L6-v2\u0026#34;) return tokenizer, model S·ª≠ d·ª•ng sentence-transformers cho embedding nh·∫•t qu√°n Cache model ƒë·ªÉ tƒÉng hi·ªáu su·∫•t H·ªó tr·ª£ nhi·ªÅu lo·∫°i d·ªØ li·ªáu 2. Pipeline X·ª≠ l√Ω D·ªØ li·ªáu def compute_embedding(text, tokenizer, model): tokens = tokenizer(text, return_tensors=\u0026#34;pt\u0026#34;, padding=True, truncation=True) with torch.no_grad(): output = model(**tokens) return output.last_hidden_state.mean(dim=1).squeeze().numpy() Tokenization v·ªõi padding v√† truncation T·ªëi ∆∞u h√≥a tensor operations Mean pooling cho embedding k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh 3. Chi·∫øn l∆∞·ª£c Chia nh·ªè B·∫£ng def process_table_chunks(df, chunk_size=5): return [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)] Chia b·∫£ng th√†nh c√°c chunk qu·∫£n l√Ω ƒë∆∞·ª£c Duy tr√¨ quan h·ªá gi·ªØa c√°c d√≤ng K√≠ch th∆∞·ªõc chunk c√≥ th·ªÉ c·∫•u h√¨nh T√≠nh nƒÉng H·ªó tr·ª£ Lo·∫°i D·ªØ li·ªáu File CSV cho d·ªØ li·ªáu c√≥ c·∫•u tr√∫c JSON cho d·ªØ li·ªáu b√°n c·∫•u tr√∫c T·ª± ƒë·ªông ph√°t hi·ªán ƒë·ªãnh d·∫°ng Ph√¢n t√≠ch Embedding Tr·ª±c quan h√≥a ph√¢n ph·ªëi Ph√¢n t√≠ch ƒë·ªô l·ªõn √Ånh x·∫° quan h·ªá chunk Ph√¢n t√≠ch b·∫±ng AI Nh·∫≠n di·ªán m·∫´u X√°c ƒë·ªãnh xu h∆∞·ªõng Ph√°t hi·ªán b·∫•t th∆∞·ªùng Chi ti·∫øt Tri·ªÉn khai T·∫£i v√† X·ª≠ l√Ω D·ªØ li·ªáu # X·ª≠ l√Ω CSV data = pd.read_csv(uploaded_file) chunks = process_table_chunks(data) embeddings = [] for chunk in chunks: chunk_text = chunk.to_string() embedding = compute_embedding(chunk_text, tokenizer, model) embeddings.append(embedding) # X·ª≠ l√Ω JSON data = json.load(uploaded_file) json_text = json.dumps(data, indent=2) embedding = compute_embedding(json_text, tokenizer, model) Tr·ª±c quan h√≥a v√† Ph√¢n t√≠ch # Tr·ª±c quan h√≥a embedding fig, ax = plt.subplots() embedding_norms = [np.linalg.norm(emb) for emb in embeddings] ax.hist(embedding_norms, bins=20) ax.set_title(\u0026#34;Ph√¢n ph·ªëi ƒê·ªô l·ªõn Embedding\u0026#34;) Best Practices Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu\nL√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫ßu v√†o X·ª≠ l√Ω gi√° tr·ªã thi·∫øu ph√π h·ª£p X√°c th·ª±c c·∫•u tr√∫c d·ªØ li·ªáu T·∫°o Embedding\nS·ª≠ d·ª•ng tokenization nh·∫•t qu√°n X·ª≠ l√Ω token ngo√†i t·ª´ v·ª±ng Tri·ªÉn khai x·ª≠ l√Ω l·ªói ph√π h·ª£p T·ªëi ∆∞u h√≥a Hi·ªáu su·∫•t\nX·ª≠ l√Ω h√†ng lo·∫°t cho t·∫≠p d·ªØ li·ªáu l·ªõn TƒÉng t·ªëc GPU khi c√≥ th·ªÉ Cache embedding th∆∞·ªùng xuy√™n s·ª≠ d·ª•ng "
},
{
	"uri": "//localhost:1313/vi/3-text-to-sql-business/",
	"title": "Text-to-SQL cho Doanh nghi·ªáp",
	"tags": [],
	"description": "",
	"content": "V·∫•n ƒë·ªÅ Doanh nghi·ªáp c·∫ßn chuy·ªÉn ƒë·ªïi c√¢u h·ªèi b·∫±ng ng√¥n ng·ªØ t·ª± nhi√™n th√†nh truy v·∫•n SQL c√≥ th·ªÉ ch·∫°y tr·ª±c ti·∫øp tr√™n c∆° s·ªü d·ªØ li·ªáu c·ªßa h·ªç, cho ph√©p ng∆∞·ªùi d√πng kh√¥ng chuy√™n v·ªÅ k·ªπ thu·∫≠t truy c·∫≠p v√† ph√¢n t√≠ch d·ªØ li·ªáu hi·ªáu qu·∫£ m√† kh√¥ng c·∫ßn bi·∫øt SQL.\nKi·∫øn tr√∫c Gi·∫£i ph√°p 1. Ph√¢n t√≠ch Schema C∆° s·ªü d·ªØ li·ªáu def connect_to_database(self, db_path): self.conn = sqlite3.connect(db_path) cursor = self.conn.cursor() # Tr√≠ch xu·∫•t th√¥ng tin schema cursor.execute(\u0026#34;SELECT name FROM sqlite_master WHERE type=\u0026#39;table\u0026#39;;\u0026#34;) tables = cursor.fetchall() schema = {} for table in tables: table_name = table[0] cursor.execute(f\u0026#34;PRAGMA table_info({table_name});\u0026#34;) columns = cursor.fetchall() schema[table_name] = [col[1] for col in columns] Tr√≠ch xu·∫•t schema t·ª± ƒë·ªông √Ånh x·∫° b·∫£ng v√† c·ªôt Ph√°t hi·ªán quan h·ªá 2. Chuy·ªÉn ƒë·ªïi Ng√¥n ng·ªØ T·ª± nhi√™n sang SQL def generate_sql(self, query): prompt = f\u0026#34;\u0026#34;\u0026#34; V·ªõi schema c∆° s·ªü d·ªØ li·ªáu sau: {self.schema} Chuy·ªÉn ƒë·ªïi c√¢u h·ªèi n√†y th√†nh SQL: \u0026#34;{query}\u0026#34; \u0026#34;\u0026#34;\u0026#34; response = self.client.chat.completions.create( model=\u0026#34;gpt-4\u0026#34;, messages=[ {\u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;B·∫°n l√† m·ªôt chuy√™n gia SQL.\u0026#34;}, {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: prompt} ], temperature=0.3 ) Chuy·ªÉn ƒë·ªïi b·∫±ng GPT-4 T·∫°o truy v·∫•n d·ª±a tr√™n schema Th·ª±c thi quy t·∫Øc t·ªëi ∆∞u h√≥a 3. Th·ª±c thi Truy v·∫•n v√† K·∫øt qu·∫£ def execute_sql(self, sql): formatted_sql = sqlparse.format(sql, reindent=True, keyword_case=\u0026#39;upper\u0026#39;) df = pd.read_sql_query(sql, self.conn) return formatted_sql, df ƒê·ªãnh d·∫°ng v√† x√°c th·ª±c SQL Th·ª±c thi truy v·∫•n an to√†n K·∫øt qu·∫£ d·∫°ng DataFrame T√≠nh nƒÉng T√≠ch h·ª£p C∆° s·ªü d·ªØ li·ªáu H·ªó tr·ª£ SQLite Ph√°t hi·ªán schema t·ª± ƒë·ªông √Ånh x·∫° quan h·ªá X·ª≠ l√Ω Truy v·∫•n Hi·ªÉu ng√¥n ng·ªØ t·ª± nhi√™n T·ªëi ∆∞u h√≥a SQL X·ª≠ l√Ω l·ªói Tr√¨nh b√†y K·∫øt qu·∫£ Hi·ªÉn th·ªã SQL ƒë·ªãnh d·∫°ng B·∫£ng d·ªØ li·ªáu t∆∞∆°ng t√°c Kh·∫£ nƒÉng xu·∫•t d·ªØ li·ªáu V√≠ d·ª• Tri·ªÉn khai # Kh·ªüi t·∫°o processor processor = TextToSQLProcessor() # K·∫øt n·ªëi c∆° s·ªü d·ªØ li·ªáu if processor.connect_to_database(\u0026#34;database.db\u0026#34;): # Nh·∫≠n c√¢u h·ªèi query = \u0026#34;T·ªïng doanh s·ªë b√°n h√†ng trong qu√Ω v·ª´a qua l√† bao nhi√™u?\u0026#34; # T·∫°o v√† th·ª±c thi SQL sql = processor.generate_sql(query) formatted_sql, results = processor.execute_sql(sql) # Hi·ªÉn th·ªã k·∫øt qu·∫£ print(formatted_sql) print(results) Best Practices T·∫°o Truy v·∫•n\nX√°c th·ª±c tham chi·∫øu schema T·ªëi ∆∞u h√≥a ph√©p JOIN Tri·ªÉn khai ki·ªÉm tra b·∫£o m·∫≠t X·ª≠ l√Ω L·ªói\nX√°c th·ª±c ƒë·∫ßu v√†o ng∆∞·ªùi d√πng X·ª≠ l√Ω l·ªói SQL m·ªôt c√°ch kh√©o l√©o Cung c·∫•p th√¥ng b√°o l·ªói r√µ r√†ng Hi·ªáu su·∫•t\nCache truy v·∫•n th∆∞·ªùng xuy√™n T·ªëi ∆∞u h√≥a t·∫≠p k·∫øt qu·∫£ l·ªõn Gi√°m s√°t th·ªùi gian th·ª±c thi "
},
{
	"uri": "//localhost:1313/vi/",
	"title": "T·∫£n M·∫°n V·ªÅ LLM GenAI",
	"tags": [],
	"description": "",
	"content": "T·∫£n M·∫°n V·ªÅ LLM GenAI Sau m·ªôt th·ªùi gian tu luy·ªán v·ªÅ LLM c≈©ng nh∆∞ GenAI, em c≈©ng t√¨m hi·ªÉu ƒë∆∞·ª£c m·ªôt s·ªë ki·∫øn th·ª©c hay c·∫ßn ch√∫ √Ω v·ªÅ LLM. Bu·ªïi workshop n√†y em s·∫Ω t·∫≠p trung v√†o m·ªôt s·ªë v·∫•n ƒë·ªÅ g·∫∑p ph·∫£i trong qu√° tr√¨nh tri·ªÉn khai LLM:\nDemo Video Xem demo tr√™n YouTube ‚ñ∂Ô∏è\nT·ªïng quan c√°c v·∫•n ƒë·ªÅ v√† gi·∫£i ph√°p X·ª≠ l√Ω Token Limitations trong LLMs: X·ª≠ l√Ω t√†i li·ªáu l·ªõn v·ªõi chunking v√† RAG. H·ªá th·ªëng Embedding Knowledge: X·ª≠ l√Ω d·ªØ li·ªáu c√≥ c·∫•u tr√∫c trong h·ªá th·ªëng multi-agent. Text-to-SQL cho Doanh nghi·ªáp: Chuy·ªÉn ƒë·ªïi ng√¥n ng·ªØ t·ª± nhi√™n th√†nh truy v·∫•n SQL. B·∫Øt ƒë·∫ßu M·ªói gi·∫£i ph√°p bao g·ªìm:\nT√†i li·ªáu chi ti·∫øt M√£ tri·ªÉn khai C√°c ph∆∞∆°ng ph√°p t·ªët nh·∫•t "
},
{
	"uri": "//localhost:1313/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]